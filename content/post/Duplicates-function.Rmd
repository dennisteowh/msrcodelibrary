---
title: "Duplicates-function"
author: "Don Pereira"
date: "8/3/2020"
output:
  html_document:
    number_sections: no
    theme: united
    toc: yes
    toc_depth: '3'
    toc_float: yes
---

The purpose of Identify_Duplicates() is to identify duplicates in a dataset based on a key identifier and remove them based on two criterias: survey completion rate and temporal precedence of the duplicates.

Using these two criterias, the function keeps the most complete survey, but where participants complete the survey to the same extent, the function keeps the earlier entry.

This function has some limitations, however, it only works with data collected from Qualtrics as it makes use of two default column names: EndDate and Progress.

I created this function to speed up my arduous data cleaning process of multiple large datasets. I am new to R and I just needed a function to get the job done. So please pardon the lack of elegance in the code.

Lets get started. In the first section, I will demonstrate how to use the function by showing you it's inputs and outputs. That is pretty much all you need to know for you to use it. For those enthused by technical details you may read further to the section 2 where i heavily comment each step of the function.

<p style="font-size:8pt; font-style:italic">
Disclaimer: I am 100% sure there is a simpler way to go about this but I enjoyed—and continue to enjoy—the problem-solving process that comes with programming. I hope that by detailing the steps in this function you may learn not only about a new function that can hopefully sort out your duplicated entries problem, but also learn something you can do—or don't—to your own code.
</p>

# Section 0: Load our libraries.

The function just requires 1 library: dplyr

```{r}
library(dplyr)
```

```{r, echo = FALSE}
Identify_Duplicates <- function (dfnew, identifier, filter, var_4 = NULL, var_5 = NULL, save = FALSE) {
  df_temp <- NULL
  
  dfremovedrows <- dfnew[dfnew[, filter] %in% 1, ]
  dfnew <- dfnew[dfnew[, filter] %in% 0, ]
  df_temp <- dfnew %>%
    arrange(dfnew[, identifier]) 
  
  dup_id_pos <- NULL
  dup_id_pos <- which(duplicated (df_temp[, identifier])) # position of the duplicated entry
  dup_id_vec <- NULL
  dup_id_vec <- as.vector(unlist(df_temp [dup_id_pos, identifier])) # identifier's that are duplicates
  dup_id_pos2 <- NULL
  dup_id_pos2 <- which (df_temp[, identifier] %in% dup_id_vec) # Keep identifiers that match the duplicate identifier list
  dup_id <- NULL
  dup_id <- df_temp [dup_id_pos2, ] %>%
    select (identifier, "EndDate", "Progress", filter, var_4, var_5) ## Extract important info, and store all duplicates
  
  ## Print out list of duplicates
  message(paste("\n", knitr::kable(table(dup_id[, identifier]))))
  
  toBeRemoved <- NULL
  if (dim(dup_id)[1] == 0) {
    message(paste("No duplicates found"))
    df_temp <- rbind(df_temp, dfremovedrows)
    return (df_temp)
  } else {
    for (i in 1:(nrow(dup_id) - 1)) {
      row <- NULL
      
      if (dup_id[i, identifier] == dup_id[i+1, identifier]) {
        if (dup_id[i, "Progress"] > dup_id[i+1, "Progress"]) {
          row <- dup_id_pos2[i+1]
        }
        else if (dup_id[i, "Progress"] < dup_id[i+1, "Progress"]) {
          row <- dup_id_pos2[i]
        } 
        else if (dup_id[i, "Progress"] == dup_id[i+1, "Progress"]) {
          if (dup_id[i, "EndDate"] > dup_id[i+1, "EndDate"]) {
            row <- dup_id_pos2[i]
          }
          else {
            row <- dup_id_pos2[i+1]
          }
        }
      }
      toBeRemoved <- c(toBeRemoved,row)
    }
  }
  
  df_temp[toBeRemoved, filter] <- 1
  
  dup_id <- df_temp [dup_id_pos2, ] %>%
    select (identifier, "EndDate", "Progress", filter, var_4, var_5) 
  
  if (save){
    write.csv(dup_id, "~/Desktop/Website/duplicates.csv") 
  }
  
  df_temp <- rbind(df_temp, dfremovedrows)
  
  Identify_Duplicates(df_temp, identifier, filter, var_4, var_5, save)
}
```

# Section 1: How to use the function
Here, I am going to run through a dataset step by step and demonstrate the function for you.

Let's first read in a dataset and populate it with some data.

The dataset only has 3 variables, a start date and end date (with a timestamp), and the completion rate of the survey "Progress". We don't actually use start date but my OCD couldn't handle having only an end without a beginning >_<

We also populate the dataset with random data for 6 variables to simulate a typical dataset.

```{r}
## Read in the data
df <- read.csv("Data1.csv")

head(df)

## Populate the dataset with random data for 6 variables to simulate a typical dataset
for (i in 1:6){
  df[, i+3] <- sample(1:6, 531, replace = TRUE) 
}

## So now we have 9 variables
head(df)
```

We will now create another variable that will be used as an identifier. Typically in a survey, you might ask participants for identifiable data such as their Student ID, or generate a randomized unique code to track them. This is most pertinent when collecting longitudinal data and tracking participants across multiple time points is critical. The range 1 - 10000 is arbitrary and can be replaced with any range of numbers. Identify_Duplicates() will also work on alpha-numeric IDs.

```{r}
set.seed(1975792)
df$ID <- as.numeric(sample(1:10000, 531, replace = TRUE))
head(df)
```

Lets see if there are duplicates. If you look at the ID column, you will notice some of the IDs appear in multiple rows. ID = 327, for example, appears twice. You can also use table(df$ID) to view this. Any ID that has more than 1 on it has a duplicate, but it looked very clunky when I used it here so I refrained from doing so. 

```{r}
dplyr::arrange(df, ID)
```

We now need to figure out which of the entries to keep. Here comes Identify_Duplicates() to save the day. But before we use it, we need to do some setting up for it to work properly.

We need to ensure that the progress variable is of numeric datatype and the EndDate variable is a POSIXct datatype. This will allow us to compare them using arithmetic operators "-+/*".

```{r}
df$EndDate <- as.POSIXct(df$EndDate)
df$Progress <- as.numeric(df$Progress)
```

Identify_Duplicates() does not remove the duplicates for you, but merely flags them up. So we will need to create a variable for it to indicate the duplicates for you. Hence, the creation of the removeVariable variable. I set the default to be 0 because the function will indicate a 1 if the item is a duplicate and should be removed. You can set this default number to be anything other than 1.

```{r}
df$removeVariable <- 0
```

Identify_Duplicates(dataframe, identifier variable, removeVariable, save = FALSE) takes in 3 arguments in order to run, but also makes use of Progress and EndDate behind the scenes. The save argument tells the function to save a .csv file of just the variables that were duplicated and the final decision the function made for easy reviewing. By default, the save argument is set to FALSE.

The function will also conveniently print to the console a list of IDs and how many times they were found.

```{r}
df.2 <- Identify_Duplicates(df, "ID", "removeVariable", save = TRUE)
```

## Section 1.1: Reviewing the results of Identify_Duplicates()
If we look at the first 2 rows, we see these are duplicates IDs. On the EndDate column, we can see this participant took the survey twice, once on 10 April completing 100% and once more on 25 April completing 100. Here, the decision is obvious, we keep the earlier survey and flag the other.

Lets look at ID = 2963 (rows 5 and 6). This participant took the survey once on 10 April completing 6% and another on 12 April completing 100%. Once again, pretty obvious, we keep the most complete entry and flag the other entry.

```{r}
## Here I read in the .csv file so its easier for us to visualize what took place
df.remDups <- read.csv("~/Desktop/Website/duplicates.csv", header = TRUE)

df.remDups
```

## Section 1.2: The End. Or is it?
Thats it. That is all you need to know to use the function. Many of us use Qualtrics for data collection and may have different criterias on what to use to clean data. This is just one of them, and if you agree with how the function flags duplicates, please feel free to use it.

# Section 2: The function

Wow, if you are reading this, do know that I never thought anyone would make it this far into this post.

```{r}
Identify_Duplicates <- function (dfnew, identifier, filter, var_4 = NULL, var_5 = NULL, save = FALSE) {
  # Create a temporary variable to store the data
  df_temp <- NULL
  
  # If there were any previous flags in the "filter" (our removeVariable variable) store them elsewhere and remove them.
  dfremovedrows <- dfnew[dfnew[, filter] %in% 1, ]
  dfnew <- dfnew[dfnew[, filter] %in% 0, ]
  
  # Take the remaining dataset and sort by identifier
  df_temp <- dfnew %>%
    arrange(dfnew[, identifier]) 
  
  # Here we extract out the IDs that are duplicated
  
  ## Store the position of the duplicated entry
  dup_id_pos <- NULL
  dup_id_pos <- which(duplicated (df_temp[, identifier])) 
  
  ## Store the identifiers that are duplicates
  dup_id_vec <- NULL
  dup_id_vec <- as.vector(unlist(df_temp [dup_id_pos, identifier])) 
  
  ## Keep identifiers that match the duplicate identifier list
  dup_id_pos2 <- NULL
  dup_id_pos2 <- which (df_temp[, identifier] %in% dup_id_vec) 
  
  ## Extract the duplicated entries and store all duplicates. We will use this dataset to apply our logical criteria for identifiying which entry to flag.
  dup_id <- NULL
  dup_id <- df_temp [dup_id_pos2, ] %>%
    select (identifier, "EndDate", "Progress", filter) 
  
  # Print out list of duplicates to the console
  message(paste("\n", knitr::kable(table(dup_id[, identifier]))))
  
  # Create a variable to store the position of all the flagged entries
  toBeRemoved <- NULL
  
  # This first if statement checks if there are any duplicates. If there aren't any, tell the user, add back the flagged rows we removed before, return the dataset and end the function.
  if (dim(dup_id)[1] == 0) {
    message(paste("No duplicates found"))
    df_temp <- rbind(df_temp, dfremovedrows)
    return (df_temp)
  } else {
    ## We loop through every row in our new dataset that has all our duplicates
    for (i in 1:(nrow(dup_id) - 1)) {
      row <- NULL ## Create a variable to store the rows to flag
      
      ## The magic: we check 2 adjacent row IDs are the same, meaning to say are they duplicates?
      if (dup_id[i, identifier] == dup_id[i+1, identifier]) {
        ## if they are, store the position (i.e., row) of the entry with lower progress
        if (dup_id[i, "Progress"] > dup_id[i+1, "Progress"]) {
          row <- dup_id_pos2[i+1]
        }
        else if (dup_id[i, "Progress"] < dup_id[i+1, "Progress"]) {
          row <- dup_id_pos2[i]
        }
        ## If their progress are the same, store the position (i.e., row) of the later entry
        else if (dup_id[i, "Progress"] == dup_id[i+1, "Progress"]) {
          if (dup_id[i, "EndDate"] > dup_id[i+1, "EndDate"]) {
            row <- dup_id_pos2[i]
          }
          else {
            row <- dup_id_pos2[i+1]
          }
        }
      }
      toBeRemoved <- c(toBeRemoved,row) ## At every iteration, add the position of the flagged rows into a vector
    }
  }
  
  ## flag the entries
  df_temp[toBeRemoved, filter] <- 1
  
  ## If you want to review which rows were flagged/not-flagged you can indicate save = true
  if (save){
    ## Extract the variables with the updated filer and any other important info for printing
    dup_id <- df_temp [dup_id_pos2, ] %>%
    select (identifier, "EndDate", "Progress", filter, var_4, var_5) 
  
    write.csv(dup_id, "~/Desktop/Website/duplicates.csv") 
  }
  
  ## Add back all the rows we removed earlier
  df_temp <- rbind(df_temp, dfremovedrows)
  
  ## Up to this point, the function only checks 2 rows and flags one of the two. Meaning to say, if we have multiple duplicates of the same identifier in our dataset, the function (may) flag only 1. I say "may" because there are cases where it flags duplicates perfectly fine in the first iteration. The solution I came up with was to recursively call the function until no more duplicates remain so regardless of the number of duplicates in your dataset, the function will run until all are flagged.
  Identify_Duplicates(df_temp, identifier, filter, var_4, var_5, save)
}
```
